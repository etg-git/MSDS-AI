# streamlit_msds_app.py
# PDF â†’ Text/OCR(auto) â†’ Visual-Order Normalize â†’ Section Split
# â†’ ì„¹ì…˜2(H/P ì „ì²´, ë¶„ë¥˜í‘œ) â†’ ì„¹ì…˜3(êµ¬ì„±ì„±ë¶„) â†’ ì„¹ì…˜9(ë¬¼ë¦¬Â·í™”í•™)
# â†’ ì„¹ì…˜15(ê·œì œí•­ëª©) â†’ GHS ê·¸ë¦¼ë¬¸ì(GIF) & íŠ¸ë¦¬ê±° Hì½”ë“œ
# íŒŒì¼ë³„ ê²°ê³¼ + ì „ì²´ ì§‘ê³„/CSV ë‹¤ìš´ë¡œë“œ

import os
import re
import io
import tempfile
import streamlit as st
import pandas as pd

# ---- í”„ë¡œì íŠ¸ ëª¨ë“ˆ (ê¸°ì¡´ ê²ƒë“¤ ê·¸ëŒ€ë¡œ ì‚¬ìš©)
from msds_text_extractor import extract_pdf_text_auto
try:
    from utils.robust_pdf_text import extract_pdf_text_visual_order
    _HAS_VISUAL = True
except Exception:
    _HAS_VISUAL = False

from section.msds_section_splitter import split_sections_auto
from field.legal_reg_table import build_legal_table
from field.hazard_class_table import build_hazard_class_table
from field.physchem_extractor import extract_physchem
from field.ghs_pictogram_mapper import map_hcodes_to_pictos_detailed
from field.hp_simple import extract_hp_simple

# í´ë°±ìš©(ìˆëŠ” ê²½ìš°ë§Œ ì‚¬ìš©)
try:
    from field.composition_extractor import extract_composition as _fallback_comp_extractor
    _HAS_COMP_FALLBACK = True
except Exception:
    _HAS_COMP_FALLBACK = False

# ---- íŒ¨í„´
CAS_RE = r"\b(\d{2,7}-\d{2}-\d)\b"
H_RE   = r"\bH\d{3}[A-Z]?\b"
P_RE   = r"\bP\d{3}[A-Z]?(?:\+P\d{3}[A-Z]?)?\b"
MSDS_NO_RE = r"\b(?:MSDS|SDS)\s*(?:No\.?|ë²ˆí˜¸|#)\s*[:ï¼š]?\s*([A-Za-z0-9\-\._]+)"

# ---- ì´ë¯¸ì§€ í´ë”(í”„ë¡œì íŠ¸ ìƒëŒ€ê²½ë¡œ)
IMAGE_DIR = os.path.join("msds", "image")  # ì˜ˆ: msds/image/GHS01.gif

st.set_page_config(page_title="MSDS Batch Extractor", layout="wide")
st.title("MSDS Batch Uploader & Extractor")
st.caption("ì—¬ëŸ¬ PDFë¥¼ í•œ ë²ˆì— ì˜¬ë ¤ ì„¹ì…˜2/3/9/15, GHS ê·¸ë¦¼ë¬¸ì, H/P ë¼ì¸, ë©”íƒ€(ì œí’ˆëª…Â·íšŒì‚¬Â·MSDS NoÂ·CAS)ê¹Œì§€ ì¼ê´„ ì¶”ì¶œí•©ë‹ˆë‹¤.")

# ---- ìœ í‹¸
def extract_basic_fields(text: str):
    H   = sorted(set(re.findall(H_RE, text)))
    P   = sorted(set(re.findall(P_RE, text)))
    CAS = sorted(set(re.findall(CAS_RE, text)))
    return H, P, CAS

_MSDS_ANCHORS = [
    "ì œí’ˆ ë° íšŒì‚¬ ì‹ë³„","ìœ í•´ì„±","ìœ„í—˜ì„±","êµ¬ì„±ì„±ë¶„","ì‘ê¸‰ì¡°ì¹˜","í­ë°œ","í™”ì¬","ëˆ„ì¶œì‚¬ê³ ",
    "ì·¨ê¸‰ ë° ì €ì¥","ë…¸ì¶œë°©ì§€ ë° ê°œì¸ë³´í˜¸êµ¬","ë¬¼ë¦¬í™”í•™ì  íŠ¹ì„±","ì•ˆì •ì„± ë° ë°˜ì‘ì„±","ë…ì„±",
    "í™˜ê²½ì— ë¯¸ì¹˜ëŠ” ì˜í–¥","íê¸°","ìš´ì†¡","ë²•ì  ê·œì œ","ê·œì œ ì •ë³´","ê¸°íƒ€ ì°¸ê³ ì‚¬í•­",
    "identification","hazards","composition","first-aid","firefighting","accidental release",
    "handling and storage","exposure controls","physical","stability and reactivity","toxicological",
    "ecological","disposal","transport","regulatory","other information",
]

def _score_headers(t: str) -> int:
    if not t: return -1
    low = t.lower()
    return sum(1 for k in _MSDS_ANCHORS if k.lower() in low)

def _jaccard(a: str, b: str) -> float:
    def trigrams(s):
        s = re.sub(r"\s+", " ", s.strip())
        return {s[i:i+3] for i in range(max(0, len(s)-2))}
    if not a or not b: return 0.0
    A, B = trigrams(a[:20000]), trigrams(b[:20000])
    if not A or not B: return 0.0
    inter = len(A & B); union = len(A | B)
    return inter/union if union else 0.0

def _csv_bytes(df: pd.DataFrame) -> bytes:
    buf = io.StringIO()
    df.to_csv(buf, index=False, encoding="utf-8-sig")
    return buf.getvalue().encode("utf-8-sig")

def _auto_height(s: str) -> int:
    if not s: return 140
    n_lines = s.count("\n") + 1
    by_lines = 22 * min(35, n_lines)
    by_chars = int(min(900, max(140, len(s) * 0.04)))
    return max(140, min(900, max(by_lines, by_chars)))

# ---- ì„¹ì…˜ í—¬í¼
def _sec_text(sections: dict, keys=('physical_chemical', 'composition', 'identification')):
    return {k: sections.get(k, {}).get("text", "") for k in keys}

def _first_nonempty(*vals):
    for v in vals:
        if v and str(v).strip():
            return v
    return ""

# ---- ë©”íƒ€ ì¶”ì¶œ(ì œí’ˆëª…/íšŒì‚¬/ê³µê¸‰ì‚¬/MSDS No/ëŒ€í‘œ CAS)
def extract_meta(text: str, sections: dict) -> dict:
    sec = _sec_text(sections, keys=('identification','composition'))
    ident = sec.get('identification', "") or ""
    comp  = sec.get('composition', "") or ""

    product = (
        _search_label_value(ident, ["ì œí’ˆëª…","ì œí’ˆ ì‹ë³„ì","í‘œì§€ëª…","Product name","Product identifier","Trade name"]) or
        _search_label_value(text,  ["ì œí’ˆëª…","ì œí’ˆ ì‹ë³„ì","Product name","Product identifier","Trade name"])
    )

    supplier = (
        _search_label_value(ident, ["ì œì¡°ì‚¬","íšŒì‚¬ëª…","ê³µê¸‰ì‚¬","ìˆ˜ì…ì‚¬","Manufacturer","Supplier","Company name","Importer"]) or
        _search_label_value(text,  ["ì œì¡°ì‚¬","íšŒì‚¬ëª…","ê³µê¸‰ì‚¬","Supplier","Manufacturer","Company"])
    )

    msds_no = (
        _search_regex_group(ident, MSDS_NO_RE) or
        _search_regex_group(text,  MSDS_NO_RE)
    )

    cas_all = re.findall(CAS_RE, comp) or re.findall(CAS_RE, text)
    rep_cas = cas_all[0] if cas_all else ""

    return {
        "product_name": (product or "").strip(),
        "supplier": (supplier or "").strip(),
        "msds_no": (msds_no or "").strip(),
        "representative_cas": rep_cas
    }

def _search_label_value(block: str, labels: list) -> str:
    if not block: return ""
    for lb in labels:
        m = re.search(rf"{re.escape(lb)}\s*[:ï¼š]\s*(.+)", block, re.I)
        if m:
            return m.group(1).strip()
    for lb in labels:
        for line in block.splitlines():
            if re.search(rf"\b{re.escape(lb)}\b", line, re.I):
                parts = re.split(r"\s{2,}", line.strip())
                if len(parts) >= 2:
                    return parts[-1].strip()
    return ""

def _search_regex_group(block: str, pattern: str) -> str:
    if not block: return ""
    m = re.search(pattern, block, re.I)
    return m.group(1).strip() if m else ""

# ---- êµ¬ì„±ì„±ë¶„(ì„¹ì…˜3) í‘œ ì¶”ì¶œ (ê°„ë‹¨/íŠ¼íŠ¼ + í´ë°±)
def extract_composition_table(text: str, sections: dict) -> pd.DataFrame:
    sec_comp = sections.get("composition", {}).get("text", "") or text
    rows = []
    for ln in sec_comp.splitlines():
        ln_strip = ln.strip()
        if not ln_strip:
            continue
        m = re.search(CAS_RE, ln_strip)
        if not m:
            continue
        cas = m.group(1)
        conc_m = re.search(r"(\d{1,3}(?:\.\d+)?\s*%|\d{1,4}\s*ppm|\d{1,4}\s*mg/m\^?3|\d{1,3}\s*-\s*\d{1,3}\s*%)", ln_strip, re.I)
        name = ln_strip[:m.start()].strip(" -:\t|Â·â€¢")
        name = re.sub(r"\s{2,}", " ", name)
        ec = ""
        ec_m = re.search(r"\b(EINECS|EC|ë“±ë¡ë²ˆí˜¸|Registration)\b[:ï¼š]?\s*([A-Za-z0-9\-\.]+)", ln_strip[m.end():], re.I)
        if ec_m:
            ec = ec_m.group(2)
        rows.append({"name": name, "cas": cas, "concentration": (conc_m.group(1) if conc_m else ""), "ec_no": ec})

    if rows:
        out, seen = [], set()
        for r in rows:
            if r["cas"] in seen:
                continue
            seen.add(r["cas"]); out.append(r)
        return pd.DataFrame(out)

    # í´ë°± ì¶”ì¶œê¸°(ìˆìœ¼ë©´)
    if _HAS_COMP_FALLBACK:
        try:
            comp_rows, comp_missed, comp_logs = _fallback_comp_extractor(text=text, comp_section_text=sections.get("composition", {}).get("text", ""))
            df = pd.DataFrame(comp_rows) if comp_rows else pd.DataFrame(columns=["name","cas","concentration","ec_no"])
            return df
        except Exception:
            pass

    return pd.DataFrame(columns=["name","cas","concentration","ec_no"])

# ========================== ë©€í‹° ì—…ë¡œë” ==========================
files = st.file_uploader("MSDS PDF ë‹¤ì¤‘ ì—…ë¡œë“œ", type=["pdf"], accept_multiple_files=True)
if not files:
    st.info("PDF ì—¬ëŸ¬ ê°œë¥¼ ì„ íƒí•´ ì—…ë¡œë“œí•˜ì„¸ìš”.")
    st.stop()

# ê²°ê³¼ ëˆ„ì  ì €ì¥ì†Œ
summary_rows = []
agg_hazard = []     # ì„¹ì…˜2 ë¶„ë¥˜/êµ¬ë¶„ í†µí•©
agg_legal = []      # ì„¹ì…˜15 ê·œì œí•­ëª© í†µí•©
agg_phys = []       # ì„¹ì…˜9 ë¬¼ë¦¬í™”í•™ í†µí•©
agg_hp_lines = []   # H/P ì „ì²´ ë¼ì¸
agg_comp = []       # ì„¹ì…˜3 êµ¬ì„±ì„±ë¶„ í†µí•©
agg_meta = []       # ê¸°ë³¸ ë©”íƒ€ í†µí•©

progress = st.progress(0)
status = st.empty()

for idx, file in enumerate(files, start=1):
    status.write(f"[{idx}/{len(files)}] ì²˜ë¦¬ ì¤‘: {file.name}")

    # ì„ì‹œ ì €ì¥
    tmp_dir = tempfile.mkdtemp(prefix="msds_")
    pdf_path = os.path.join(tmp_dir, file.name)
    with open(pdf_path, "wb") as f:
        f.write(file.getbuffer())

    # í…ìŠ¤íŠ¸ ì¶”ì¶œ (auto)
    res = extract_pdf_text_auto(
        file_bytes=open(pdf_path, "rb").read(),
        dpi=300,
        lang="kor+eng",
        tessdata_dir=None,
    )
    text_auto = (getattr(res, "merged_text", None) or "").strip()

    # ì‹œê° ìˆœì„œ ë³´ì •(ê°€ëŠ¥ ì‹œ)
    text_visual, visual_err = "", None
    if _HAS_VISUAL:
        try:
            text_visual = extract_pdf_text_visual_order(pdf_path) or ""
        except Exception as e:
            visual_err = f"visual-order ì‹¤íŒ¨: {e}"

    # íœ´ë¦¬ìŠ¤í‹± ì„ íƒ
    score_auto   = _score_headers(text_auto)
    score_visual = _score_headers(text_visual) if text_visual else -1
    len_auto     = len(text_auto)
    len_visual   = len(text_visual)
    overlap      = _jaccard(text_visual, text_auto) if (text_visual and text_auto) else 0.0

    use_visual = False
    if text_visual:
        cond_len   = (len_visual >= max(400, 0.9 * len_auto))
        cond_head  = (score_visual >= score_auto)
        cond_head2 = (score_visual >= score_auto + 2)
        cond_diff  = (overlap <= 0.4 and (len_visual > len_auto*0.85) and score_visual >= score_auto)
        use_visual = (cond_len and cond_head) or cond_head2 or cond_diff

    text_src = "visual" if (use_visual and text_visual) else "auto"
    text = text_visual if (text_src == "visual") else text_auto

    # ê¸°ë³¸ íŒ¨í„´
    H, P, CAS = extract_basic_fields(text)

    # ì„¹ì…˜ ë¶„ë¦¬
    sections, sec_logs, template = split_sections_auto(text)

    # ì„¹ì…˜2: H/P ë¼ì¸(ì „ì²´)
    hp_simple = extract_hp_simple(text, sections)

    # ì„¹ì…˜2: ë¶„ë¥˜/êµ¬ë¶„ í‘œ
    hz_df = pd.DataFrame()
    try:
        hz_rows = build_hazard_class_table(text)
        hz_df = pd.DataFrame(hz_rows)
        if not hz_df.empty:
            hz_df["file"] = file.name
            agg_hazard.append(hz_df)
    except Exception:
        pass

    # ì„¹ì…˜3: êµ¬ì„±ì„±ë¶„ í‘œ
    comp_df = extract_composition_table(text, sections)
    if not comp_df.empty:
        comp_df["file"] = file.name
        agg_comp.append(comp_df)

    # ì„¹ì…˜9: ë¬¼ë¦¬Â·í™”í•™ (ì„¹ì…˜9 ë³¸ë¬¸ ìš°ì„ , ì—†ìœ¼ë©´ ì „ì—­)
    sec9_text = sections.get("physical_chemical", {}).get("text", "")
    if not sec9_text:
        for k, v in sections.items():
            if k in ("9","sec9","section9"):
                sec9_text = v.get("text", ""); break
    pc_target_text = sec9_text if sec9_text.strip() else text
    phys_result, phys_log = extract_physchem(pc_target_text)
    if phys_result:
        for k, v in phys_result.items():
            row = {"file": file.name, "key": k}
            if isinstance(v, dict):
                row.update({
                    "raw": v.get("raw",""),
                    "value": v.get("value",""),
                    "low": v.get("low",""),
                    "high": v.get("high",""),
                    "cmp": v.get("cmp",""),
                    "unit": v.get("unit",""),
                })
            agg_phys.append(row)

    # ì„¹ì…˜15: ê·œì œì‚¬í•­
    legal_df = pd.DataFrame()
    try:
        legal_rows = build_legal_table(text)
        legal_df = pd.DataFrame(legal_rows)
        if not legal_df.empty:
            legal_df["file"] = file.name
            agg_legal.append(legal_df)
    except Exception:
        pass

    # GHS ê·¸ë¦¼ë¬¸ì
    ghs_details, picto_list = [], []
    try:
        ghs_details = map_hcodes_to_pictos_detailed(H) if H else []
        picto_list = [d.get("pictogram") for d in ghs_details] if ghs_details else []
    except Exception:
        pass

    # ë©”íƒ€(ì œí’ˆëª…/íšŒì‚¬/MSDS No/ëŒ€í‘œ CAS)
    meta = extract_meta(text, sections)
    meta["file"] = file.name
    meta["text_source"] = text_src
    meta["auto_len"] = len_auto
    meta["visual_len"] = len_visual
    meta["header_score_auto"] = score_auto
    meta["header_score_visual"] = score_visual
    meta["overlap"] = f"{overlap:.2f}"
    agg_meta.append(meta)

    # ìš”ì•½ í–‰
    summary_rows.append({
        "file": file.name,
        "product_name": meta.get("product_name",""),
        "supplier": meta.get("supplier",""),
        "msds_no": meta.get("msds_no",""),
        "representative_cas": meta.get("representative_cas",""),
        "text_source": text_src,
        "H_count": len(H),
        "P_count": len(P),
        "CAS_count": len(CAS),
        "hazard_class_rows": (0 if hz_df.empty else len(hz_df)),
        "legal_rows": (0 if legal_df.empty else len(legal_df)),
        "pictograms": ", ".join(sorted(set(picto_list))) if picto_list else "-",
    })

    # íŒŒì¼ë³„ ìƒì„¸(ì ‘ì´ì‹)
    with st.expander(f"ğŸ“„ {file.name} â€” ìƒì„¸ ë³´ê¸°", expanded=False):
        # ë©”íƒ€
        st.caption("ê¸°ë³¸ ë©”íƒ€")
        meta_cols = st.columns(4)
        meta_cols[0].metric("ì œí’ˆëª…", meta.get("product_name","") or "-")
        meta_cols[1].metric("íšŒì‚¬/ì œì¡°ì‚¬", meta.get("supplier","") or "-")
        meta_cols[2].metric("MSDS/SDS No", meta.get("msds_no","") or "-")
        meta_cols[3].metric("ëŒ€í‘œ CAS", meta.get("representative_cas","") or "-")
        st.caption(f"í…ìŠ¤íŠ¸ ì†ŒìŠ¤: {text_src} (auto_len={len_auto}, visual_len={len_visual}, hdr_auto={score_auto}, hdr_visual={score_visual}, overlapâ‰ˆ{overlap:.2f})")

        # ê¸°ë³¸ íŒ¨í„´/HÂ·P ë¼ì¸
        c1, c2 = st.columns(2)
        with c1:
            st.caption("ìœ í•´â€§ìœ„í—˜ë¬¸êµ¬(H) ì „ì²´")
            st.text_area(f"H-lines-{file.name}", extract_hp_simple(text, sections).get("hazard_text","") or "(ì—†ìŒ)", height=200, key=f"h_{file.name}")
        with c2:
            st.caption("ì˜ˆë°©ì¡°ì¹˜ë¬¸êµ¬(P) ì „ì²´")
            st.text_area(f"P-lines-{file.name}", extract_hp_simple(text, sections).get("precaution_text","") or "(ì—†ìŒ)", height=200, key=f"p_{file.name}")

        # ì„¹ì…˜2 ë¶„ë¥˜/êµ¬ë¶„
        st.caption("ì„¹ì…˜2 ë¶„ë¥˜/êµ¬ë¶„")
        if not hz_df.empty:
            st.dataframe(hz_df, use_container_width=True, hide_index=True)
        else:
            st.info("ë¶„ë¥˜/êµ¬ë¶„ í•­ëª© ì—†ìŒ")

        # ì„¹ì…˜3 êµ¬ì„±ì„±ë¶„
        st.caption("ì„¹ì…˜3 êµ¬ì„±ì„±ë¶„")
        if not comp_df.empty:
            st.dataframe(comp_df, use_container_width=True, hide_index=True)
        else:
            st.info("êµ¬ì„±ì„±ë¶„ í‘œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

        # ì„¹ì…˜9
        st.caption("ì„¹ì…˜9 ë¬¼ë¦¬Â·í™”í•™ (í•µì‹¬ ì¶”ì¶œ)")
        phys_df_one = pd.DataFrame([r for r in agg_phys if r.get("file")==file.name])
        if not phys_df_one.empty:
            st.dataframe(phys_df_one, use_container_width=True, hide_index=True)
        else:
            st.info("ì„¹ì…˜ 9ì—ì„œ ì¶”ì¶œëœ í•­ëª© ì—†ìŒ")

        # ì„¹ì…˜15
        st.caption("ì„¹ì…˜15 ê·œì œì‚¬í•­")
        if not legal_df.empty:
            st.dataframe(legal_df, use_container_width=True, hide_index=True)
        else:
            st.info("ê·œì œì‚¬í•­ í•­ëª© ì—†ìŒ")

        # GHS ê·¸ë¦¼ë¬¸ì(ì¸ë„¤ì¼)
        if ghs_details:
            st.caption("GHS ê·¸ë¦¼ë¬¸ì")
            cols = st.columns(min(4, len(ghs_details)))
            for i, item in enumerate(ghs_details):
                p = item["pictogram"]; img_path = os.path.join(IMAGE_DIR, f"{p}.gif")
                with cols[i % len(cols)]:
                    try:
                        st.image(img_path, width=80, caption=p)
                    except Exception:
                        st.write(p)

    # HP ë¼ì¸ í†µí•©
    if hp_simple.get("hazard_text"):
        agg_hp_lines.append({"file": file.name, "type": "H", "text": hp_simple["hazard_text"]})
    if hp_simple.get("precaution_text"):
        agg_hp_lines.append({"file": file.name, "type": "P", "text": hp_simple["precaution_text"]})

    progress.progress(idx / len(files))

# ===== ì „ì²´ ì§‘ê³„ =====
st.subheader("ğŸ“Š ì „ì²´ ìš”ì•½ / Summary")
summary_df = pd.DataFrame(summary_rows)
st.dataframe(summary_df, use_container_width=True, hide_index=True)
st.download_button("CSV ë‹¤ìš´ë¡œë“œ (ìš”ì•½)",
                  data=_csv_bytes(summary_df),
                  file_name="summary_msds_batch.csv",
                  mime="text/csv")

# ë©”íƒ€ í†µí•©
st.subheader("ë©”íƒ€ í†µí•© (ì œí’ˆëª…/íšŒì‚¬/MSDS No/CAS/í…ìŠ¤íŠ¸ì†ŒìŠ¤)")
meta_df = pd.DataFrame(agg_meta)
if not meta_df.empty:
    st.dataframe(meta_df, use_container_width=True, hide_index=True)
    st.download_button("CSV ë‹¤ìš´ë¡œë“œ (ë©”íƒ€ í†µí•©)",
                      data=_csv_bytes(meta_df),
                      file_name="meta_all.csv",
                      mime="text/csv")
else:
    st.info("ë©”íƒ€ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

# ì„¹ì…˜3 í†µí•©
st.subheader("ì„¹ì…˜ 3 í†µí•© í‘œ (êµ¬ì„±ì„±ë¶„)")
if agg_comp:
    all_comp = pd.concat(agg_comp, ignore_index=True)
    st.dataframe(all_comp, use_container_width=True, hide_index=True)
    st.download_button("CSV ë‹¤ìš´ë¡œë“œ (ì„¹ì…˜3 í†µí•©)",
                      data=_csv_bytes(all_comp),
                      file_name="sec3_composition_all.csv",
                      mime="text/csv")
else:
    st.info("ì„¹ì…˜ 3 ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

# ì„¹ì…˜2 í†µí•©
st.subheader("ì„¹ì…˜ 2 í†µí•© í‘œ (ë¶„ë¥˜/êµ¬ë¶„)")
if agg_hazard:
    all_hz = pd.concat(agg_hazard, ignore_index=True)
    st.dataframe(all_hz, use_container_width=True, hide_index=True)
    st.download_button("CSV ë‹¤ìš´ë¡œë“œ (ì„¹ì…˜2 í†µí•©)",
                      data=_csv_bytes(all_hz),
                      file_name="sec2_hazard_classes_all.csv",
                      mime="text/csv")
else:
    st.info("ì„¹ì…˜ 2 ë¶„ë¥˜/êµ¬ë¶„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

# ì„¹ì…˜9 í†µí•©
st.subheader("ì„¹ì…˜ 9 í†µí•© í‘œ (ë¬¼ë¦¬Â·í™”í•™ í•µì‹¬)")
if agg_phys:
    all_phys = pd.DataFrame(agg_phys)
    st.dataframe(all_phys, use_container_width=True, hide_index=True)
    st.download_button("CSV ë‹¤ìš´ë¡œë“œ (ì„¹ì…˜9 í†µí•©)",
                       data=_csv_bytes(all_phys),
                       file_name="sec9_physchem_all.csv",
                       mime="text/csv")
else:
    st.info("ì„¹ì…˜ 9 ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

# ì„¹ì…˜15 í†µí•©
st.subheader("ì„¹ì…˜ 15 í†µí•© í‘œ (ê·œì œì‚¬í•­)")
if agg_legal:
    all_legal = pd.concat(agg_legal, ignore_index=True)
    st.dataframe(all_legal, use_container_width=True, hide_index=True)
    st.download_button("CSV ë‹¤ìš´ë¡œë“œ (ì„¹ì…˜15 í†µí•©)",
                      data=_csv_bytes(all_legal),
                      file_name="sec15_legal_items_all.csv",
                      mime="text/csv")
else:
    st.info("ì„¹ì…˜ 15 ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

# H/P ë¼ì¸ í†µí•©
st.subheader("H/P ë¼ì¸ í†µí•© (ì›ë¬¸ ì¤„ ë¬¶ìŒ)")
if agg_hp_lines:
    hp_df = pd.DataFrame(agg_hp_lines)
    st.dataframe(hp_df, use_container_width=True, hide_index=True)
    st.download_button("TXT ë‹¤ìš´ë¡œë“œ (H-lines ì „ì²´, íŒŒì¼ë³„ ë³‘í•©)",
                      data="\n\n".join([f"[{r['file']}] H-lines\n{r['text']}" for r in hp_df.query("type=='H'").to_dict('records')]).encode("utf-8-sig"),
                      file_name="all_H_lines.txt",
                      mime="text/plain")
    st.download_button("TXT ë‹¤ìš´ë¡œë“œ (P-lines ì „ì²´, íŒŒì¼ë³„ ë³‘í•©)",
                      data="\n\n".join([f"[{r['file']}] P-lines\n{r['text']}" for r in hp_df.query("type=='P'").to_dict('records')]).encode("utf-8-sig"),
                      file_name="all_P_lines.txt",
                      mime="text/plain")
else:
    st.info("H/P ë¼ì¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
